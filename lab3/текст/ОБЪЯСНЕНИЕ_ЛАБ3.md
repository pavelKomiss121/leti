# Лабораторная работа 3. Деревья и леса решений — подробное объяснение

## 1. Цель работы

Реализация классификатора на основе **дерева принятия решений**, исследование его свойств (в том числе переобучения), сравнение с ансамблем **случайного леса**, построение **ROC-кривых** и расчёт **AUC**, подбор гиперпараметров (глубина дерева, число деревьев в лесе).

---

## 2. Основные теоретические положения

### 2.1 Дерево решений (распознающее дерево)

**Дерево решений** — классификатор, в котором для объекта выполняется **конечная последовательность сравнений** значений признаков с пороговыми константами. От результата каждого сравнения зависит, что делать дальше: продолжать проверки или выдать ответ (метку класса).

Структура задаётся вложенными операторами вида:

- **В узлах:** `if x[j] ?? d[k] then … else …`  
  где **x** — вектор признаков объекта, **d** — пороговые значения, **??** — операция сравнения (например, ≤ или >).
- **В листьях:** `return res[h]` — возврат метки класса (один из возможных ответов).

То есть распознавание — это обход по **двоичному дереву**: в каждом узле проверяется один признак, по результату переходим в левую или правую ветвь, пока не попадём в лист с ответом.

**Узлы и листья:**
- **Узел** — решающее правило (какой признак, какой порог) и подмножество наблюдений, попавших в этот узел.
- **Лист** — конечная вершина; каждому листу сопоставлен один класс; объект, дошедший до листа, получает эту метку.

Дерево можно рассматривать как **разрезание признакового пространства**: в каждом узле пространство делится гиперплоскостью вида «признак j = порог». Поэтому дерево решений — это **линейный классификатор** (граница между классами кусочно-линейная, из отрезков гиперплоскостей). Деревья применимы и для **регрессии** (в листьях — числовое значение).

### 2.2 Обучение дерева

**Обучение** — определение по размеченной выборке (обучение с учителем):

- структуры дерева (какие узлы и как связаны);
- в каждом узле: какой признак сравнивать и с каким порогом;
- в каждом листе: какой класс возвращать.

Алгоритмы построения деревьев относят к **жадным**: на каждом шаге выбирается разбиение, которое **локально** лучше всего улучшает выбранный критерий (например, уменьшает неопределённость — энтропию или индекс Джини). Выбранное разбиение не пересматривается позже, поэтому итоговое дерево не обязательно глобально оптимально, зато строится быстро.

**Почему дерево переобучается:** без ограничений алгоритм может строить узлы до тех пор, пока в листе не останется по одному объекту (или один класс). На обучающей выборке точность тогда будет 1 (или почти 1), но на новых данных такое дерево работает плохо — оно «запомнило» выборку, а не обобщило закономерность. Ограничение **глубины** (max_depth) или **минимального числа объектов в листе** (min_samples_leaf) уменьшает переобучение.

### 2.3 Ансамбли и бэггинг

**Ансамбль** — объединение нескольких алгоритмов в один: каждый даёт свой ответ, итоговый результат получают **голосованием** (классификация) или **усреднением** (регрессия). Часто ансамбль из многих «средних» моделей оказывается точнее одной сложной.

**Бэггинг (Bootstrap aggregation):**
- Исходная выборка несколько раз **выборкается с возвращением** (bootstrap) — получаются разные обучающие подвыборки.
- На каждой подвыборке обучается своя базовая модель.
- Итоговый ответ — голосование (или среднее) по всем моделям.

Так снижается **дисперсия**: если обучать одну модель на разных выборках, ошибки будут разными; усреднение ответов сглаживает эти колебания. Выбросы и шум не попадают в часть подвыборок, поэтому их влияние ослабляется. Бэггинг также **уменьшает переобучение**.

### 2.4 Случайный лес (Random Forest)

**Случайный лес** — бэггинг над **решающими деревьями** с дополнительной рандомизацией:

- Каждое дерево обучается на своей **бутстрэп-подвыборке** (выборка с возвращением из исходной).
- В каждом узле при выборе разбиения признаки рассматриваются не все, а только из **случайного подмножества** (например, √n признаков). Это делает деревья разными и снижает корреляцию их ошибок.

Для **классификации** итог — по **большинству голосов** деревьев (или по среднему вероятностей классов). Для **регрессии** — по **среднему** предсказаний деревьев.

По сравнению с одним деревом лес обычно:
- даёт более стабильную точность на тесте;
- меньше переобучается (разрыв между точностью на train и test меньше);
- лучше обобщает на нелинейно разделимых данных.

Недостатки: склонность к «рваным» границам, больше времени обучения и предсказания. Качество зависит от **числа деревьев** (n_estimators) и других гиперпараметров.

### 2.5 ROC-кривая и AUC

**ROC-кривая (Receiver Operating Characteristic)** строится для бинарного классификатора, который выдаёт **вероятность** принадлежности классу 1 (или оценку «скора»).

- По оси **X** — **FPR** (False Positive Rate) = FP / (FP + TN) — доля объектов класса 0, ошибочно отнесённых к классу 1.
- По оси **Y** — **TPR** (True Positive Rate) = TP / (TP + FN) — доля объектов класса 1, правильно отнесённых к классу 1 (это **чувствительность**).

При изменении **порога** отсечения (например, «считать класс 1, если вероятность > порога») пара (FPR, TPR) меняется. Кривая соединяет эти точки при пороге от 0 до 1. Идеальный классификатор даёт точку (0, 1); случайное угадывание — диагональ из (0,0) в (1,1).

**AUC (Area Under Curve)** — площадь под ROC-кривой. Значения от 0 до 1; чем ближе к 1, тем лучше модель разделяет классы. AUC = 0.5 соответствует случайному классификатору.

В scikit-learn: `roc_curve(y_true, y_score)` даёт массивы FPR и TPR; `roc_auc_score(y_true, y_score)` — AUC. В качестве `y_score` обычно передают вероятность класса 1 (второй столбец `predict_proba`).

### 2.6 Чувствительность, специфичность, точность

При интерпретации: **класс 0** — отсутствие признака, **класс 1** — наличие.

- **Чувствительность (Sensitivity, TPR)** = TP / (TP + FN) — доля реально положительных, правильно классифицированных как 1.
- **Специфичность (Specificity, TNR)** = TN / (TN + FP) — доля реально отрицательных, правильно классифицированных как 0.
- **Точность (Accuracy)** = (TP + TN) / (всего объектов) — доля всех верных ответов.

Формулы используются для ручного расчёта по меткам 0/1 (результат `predict`).

---

## 3. Пошаговое объяснение кода

### Шаг 1. Импорты и данные (п.1 методички)

Импортируются: numpy, matplotlib, Path, DecisionTreeClassifier (sklearn.tree), RandomForestClassifier (sklearn.ensemble), roc_curve и roc_auc_score (sklearn.metrics). При наличии — scikit_plot для визуализации ROC. Подключается lab1.DataGenerator (norm_dataset, nonlinear_dataset_5).

Создаются три набора данных:
- **Выборка А** — нормальное распределение, хорошо разделимые классы (разнесённые средние, умеренные СКО).
- **Выборка Б** — нормальное распределение, средняя степень пересечения (сближенные средние, большие СКО).
- **Выборка В** — нелинейно разделимые (nonlinear_dataset_5: овал внутри U-образной оболочки).

Для каждой выборки данные разбиваются на обучающую (70 %) и тестовую (30 %) функцией `split_70_30`. Данные из генератора уже перемешаны, поэтому первые 70 % и последние 30 % дают представительные подвыборки.

### Шаг 2–3. Обучение дерева и оценка (п.2–3)

Дерево создаётся и обучается:
`tree = DecisionTreeClassifier(random_state=0).fit(Xtrain, Ytrain)`.

Для воспроизводимости задаётся `random_state=0`. Далее вызываются:
- `tree.predict(X)` — метки классов 0/1;
- `tree.predict_proba(X)` — вероятности по классам (два столбца: P(класс 0), P(класс 1));
- `tree.score(X, Y)` — точность (доля верных ответов).

По меткам считаются чувствительность и специфичность через функцию `sensitivity_specificity(Y_true, Y_pred)`.

**Почему на train точность ≈ 1, а на test ниже:** дерево без ограничения глубины может разбивать выборку до очень мелких листьев и фактически «запоминать» обучающие объекты. На train это даёт идеальную точность, на test — худшее обобщение (переобучение).

### Шаг 4. Случайный лес (п.4)

Лес обучается аналогично:
`forest = RandomForestClassifier(random_state=0).fit(Xtrain, Ytrain)`.

Для леса также вызываются predict, predict_proba, score; считаются чувствительность и специфичность. Результаты дерева и леса выводятся в таблицы и сравниваются: обычно у леса точность на test выше и разрыв train–test меньше.

### Шаг 5. ROC-кривые и AUC (п.5)

Для тестовой выборки берутся вероятности класса 1:
- дерево: `tree_proba_test[:, 1]`;
- лес: `forest_proba_test[:, 1]`.

Истинные метки приводятся к типу 0/1 (если пришли как bool). Затем:
- `fpr, tpr, _ = roc_curve(Ytest_int, proba_class1)` — точки для построения ROC;
- `auc = roc_auc_score(Ytest_int, proba_class1)` — площадь под кривой.

Строится общий график: две кривые (дерево и лес), диагональ (0,0)–(1,1), подписи с AUC. При установленном scikit-plot можно дополнительно строить ROC через `skplt.metrics.plot_roc`.

### Шаг 6. Гистограммы для леса (п.6)

Строятся гистограммы распределения **вероятности принадлежности классу 1** (лес, predict_proba[:, 1]) отдельно для объектов с истинной меткой 0 и 1 — для обучающей и тестовой выборок по всем трём наборам (А, Б, В). По гистограммам видно, насколько уверенно модель разделяет классы и где происходит перекрытие (около 0.5 — зона неопределённости).

### Самостоятельное задание

- **Чувствительность и специфичность** — по формулам TP/(TP+FN) и TN/(TN+FP), без методов библиотеки (реализовано в `sensitivity_specificity`).
- **Нелинейно разделимые классы (выборка В)** — по таблицам и ROC видно, что дерево и лес справляются лучше линейной логистической регрессии за счёт кусочно-линейных границ.
- **Подбор глубины дерева (max_depth):** перебор значений (например, 3, 5, 10, 15, None) на выборке Б; для каждого — точность на train и test. Лучшее значение по **тестовой точности** уменьшает переобучение (train может снизиться, test — вырасти).
- **Подбор числа деревьев (n_estimators):** цикл от 1 до 300 с шагом 10; для каждого значения обучается лес, считается AUC на тесте. Выбирается n_estimators с **максимальным AUC** на тестовой выборке.

---

## 4. Три выборки и интерпретация результатов

| Выборка | Описание | Ожидание для дерева и леса |
|--------|----------|-----------------------------|
| **А** | Нормальное распределение, хорошо разделимые | Высокая точность и AUC; дерево может сильнее переобучаться (train ≈ 1, test ниже леса). |
| **Б** | Нормальное распределение, плохо разделимые | Метрики ниже; ограничение глубины дерева особенно полезно. |
| **В** | Нелинейно разделимые (овал внутри U) | Дерево и лес работают лучше линейной модели; лес обычно стабильнее по AUC и точности. |

Гистограммы леса: на А — пики у 0 и 1; на Б и В — больше перекрытие в середине (около 0.5).

---

## 5. Гиперпараметры

**Дерево (DecisionTreeClassifier):**
- **max_depth** — максимальная глубина. Меньше глубина → проще дерево, меньше переобучение, но возможен недообучение.
- **min_samples_leaf** — минимальное число объектов в листе. Больше → более грубое разбиение, меньше переобучение.
- **random_state** — для воспроизводимости (в деревьях может использоваться при выборе разбиений, если включена рандомизация).

**Лес (RandomForestClassifier):**
- **n_estimators** — число деревьев. Больше деревьев обычно увеличивают качество и AUC до выхода на плато; дальше только рост времени.
- **max_depth** — глубина каждого дерева (по умолчанию не ограничена).
- **random_state** — фиксирует бутстрэп-выборки и случайный выбор признаков.

Подбор в работе: max_depth для дерева по тестовой точности на выборке Б; n_estimators для леса по AUC на тесте (выборка А) в диапазоне 1–300 с шагом 10.

---

## 6. Краткие ответы на типичные вопросы

**Чем дерево решений отличается от логистической регрессии?**  
Дерево строит кусочно-линейную границу (гиперплоскости по порогам признаков), регрессия — одну гиперплоскость. Дерево лучше подходит для нелинейно разделимых и разномасштабных признаков, но сильнее переобучается без ограничений.

**Почему на обучающей выборке точность дерева равна (или почти равна) 1?**  
Без ограничения глубины дерево может разбивать данные до листьев с одним классом или малым числом объектов, фактически запоминая выборку. На train все объекты тогда классифицируются верно.

**Что такое бэггинг и зачем он в случайном лесе?**  
Бэггинг — обучение многих моделей на разных бутстрэп-подвыборках и усреднение (или голосование) их ответов. Это снижает дисперсию и переобучение. В лесе каждая модель — дерево; дополнительно в узлах дерева признаки берутся из случайного подмножества.

**Как читать ROC-кривую и что такое AUC?**  
ROC показывает, как при изменении порога меняются TPR (чувствительность) и FPR (1 − специфичность). Кривая тем лучше, чем ближе к верхнему левому углу (0, 1). AUC — площадь под этой кривой; 1 — идеал, 0.5 — случайное угадывание.

**Зачем ограничивать глубину дерева?**  
Чтобы уменьшить переобучение: неглубокое дерево задаёт более простую границу и лучше обобщает на новые данные, хотя на обучающей выборке точность может быть ниже.

---

Используйте этот файл для подготовки к защите и для вставки теоретических фрагментов в отчёт. После запуска `python -m lab3.main` подставьте в отчёт численные результаты из консоли (таблицы, лучшие max_depth и n_estimators, AUC).
