# Вопросы для самоконтроля — ответы (Лабораторная работа 3)

Ответы на вопросы по деревьям и лесам решений.

---

## 1. Дерево принятия решений – это линейный или нелинейный классификатор?

Дерево решений относят к **линейным** классификаторам в том смысле, что каждое решающее правило в узле — это сравнение одного признака с порогом, т.е. разбиение пространства признаков **гиперплоскостью**, перпендикулярной одной из осей. В совокупности дерево задаёт кусочно-линейную границу: область решений состоит из объединения прямоугольных областей (в 2D — прямоугольников, ограниченных прямыми, параллельными осям). При этом граница между классами не является одной прямой или одной гиперплоскостью, но строится из конечного набора таких «разрезов», поэтому в классификации методов дерево часто называют линейным (по типу элементарных правил).

---

## 2. Каким образом в двумерном пространстве признаков можно визуализировать разделяющую плоскость дерева, которая формируется набором решающих правил?

В двумерном пространстве каждое правило вида «признак j < порог» задаёт **прямую**, параллельную другой оси (вертикальную или горизонтальную). Дерево последовательно разбивает плоскость такими прямыми. Визуализировать можно так: нанести все пороговые прямые (вертикальные и горизонтальные линии на уровне порогов из узлов дерева); области между ними соответствуют листьям дерева, каждой области можно присвоить цвет или метку класса. Альтернатива — построить диаграмму рассеяния объектов и раскрасить фон по предсказанию модели в узкой сетке точек (карта решений), тогда граница между цветами и будет кусочно-линейной границей дерева.

---

## 3. Какие задачи могут решать деревья помимо классификации?

Деревья решений применяются и для **регрессии**: в листьях хранится не метка класса, а среднее (или другое значение) целевой переменной по объектам, попавшим в этот лист; предсказание для нового объекта — значение в листе, в который он попал. Таким образом, решающие деревья решают задачи как классификации (целевая переменная — категория), так и регрессии (целевая переменная — число). В scikit-learn этому соответствуют классы DecisionTreeClassifier и DecisionTreeRegressor.

---

## 4. Из каких элементов состоит дерево принятия решений?

Дерево состоит из **узлов (node)** и **листьев (leaf)**. В **узлах** находятся решающие правила: сравнение одного признака с порогом (например, «признак x_j < d»); по результату сравнения объект отправляется в левое или правое поддерево. В **листьях** дерево выдаёт итоговый ответ: в задаче классификации — метка класса, в регрессии — числовое значение. Обучение дерева определяет, какой признак и с каким порогом сравнивать в каждом узле, и какой ответ присвоить каждому листу.

---

## 5. Какие недостатки имеют деревья принятия решений и каким образом их можно сгладить?

Основные недостатки: **переобучение** (дерево может подстроиться под шум и давать идеальную точность на обучении при плохом обобщении); **нестабильность** (небольшое изменение данных может сильно изменить структуру дерева); **«ступенчатые» границы** (кусочно-постоянные предсказания в соседних областях).

Сглаживание: **ограничение глубины** дерева (max_depth) или минимального числа объектов в листе (min_samples_leaf), **регуляризация** через параметры критерия и стрижки; использование **ансамблей** (например, случайный лес), где усреднение многих деревьев снижает дисперсию и переобучение.

---

## 6. В чем преимущество использования ансамблей, в частности случайного леса?

Ансамбль объединяет множество моделей (например, деревьев), обученных с элементами случайности (разные подвыборки данных, разные подмножества признаков). Итоговое решение получается голосованием (классификация) или усреднением (регрессия). **Преимущества:** снижение дисперсии и переобучения по сравнению с одним деревом; более устойчивые предсказания; часто более высокая точность на тесте. Случайный лес конкретно комбинирует бэггинг по объектам и случайный выбор признаков при каждом разбиении, что делает деревья разнообразными и уменьшает корреляцию их ошибок.

---

## 7. Что такое бэггинг?

**Бэггинг (Bootstrap Aggregation)** — метод построения ансамбля: из исходной обучающей выборки многократно извлекаются **бутстрэп-выборки** (выборки того же размера с возвращением), на каждой обучается своя базовая модель. Итоговое предсказание — голосование (классификация) или среднее (регрессия) по всем моделям. Бэггинг снижает дисперсию: ошибки отдельных моделей, обученных на разных подвыборках, частично компенсируются при усреднении. Случайный лес — это бэггинг над решающими деревьями с дополнительной рандомизацией по признакам при построении каждого дерева.

---

## 8. Что такое гиперпараметры модели?

**Гиперпараметры** — это параметры, которые задаются **до** обучения и не оцениваются по данным в процессе обучения (в отличие от весов/коэффициентов модели). Примеры: глубина дерева (max_depth), число деревьев в лесе (n_estimators), критерий разбиения (criterion), минимальное число объектов в листе (min_samples_leaf). От выбора гиперпараметров зависит сложность и поведение модели; их подбирают по валидационной выборке или кросс-валидации (перебор или поиск по сетке).

---

## 9. Как выбрать наилучшую модель для имеющихся данных?

Выбор модели и её гиперпараметров делают на основе **оценки обобщающей способности**, а не только точности на обучающей выборке. Данные делят на обучающую, валидационную и тестовую подвыборки (или используют кросс-валидацию). На обучающей выборке обучают модель, на **валидационной** сравнивают разные модели и гиперпараметры (например, перебор max_depth или n_estimators) по выбранной метрике (точность, AUC, F1 и т.д.). Лучший вариант фиксируют; итоговую оценку качества получают один раз на **тестовой** выборке. Важно не «подглядывать» в тест при выборе модели, иначе оценка будет оптимистичной. Дополнительно учитывают интерпретируемость, скорость обучения и предсказания, объём данных.
