# Упражнение 6. Создание классификатора с помощью библиотеки PyTorch

## Краткая сводка (без кода)

**Среда.** Упражнение удобнее выполнять в Google Colab (https://colab.research.google.com/): не нужно ставить CUDA локально, есть бесплатный GPU. Локально скрипт тоже можно запустить (на CPU будет медленнее).

**Цепочка шагов.** 1) Проверка GPU (`torch.cuda.is_available()`). 2) Генерация данных `make_moons`, визуализация. 3) Разделение на train/val через `train_test_split`. 4) Загрузка в PyTorch: `TensorDataset` + `DataLoader` (батчи по batch_size). 5) Модель — линейный слой с выходом 1 (логистическая регрессия): `x @ weights + bias`, функция потерь `BCEWithLogitsLoss`, оптимизатор SGD. 6) Цикл обучения: обнуление градиентов, forward, loss, backward, step; при желании — останов по сходимости весов. 7) График loss по итерациям. 8) Функция `predict(dataloader, model)` с `torch.sigmoid` и порогом 0.5; оценка accuracy на тесте. 9) Улучшение: подбор lr, числа эпох или замена модели на сеть с нелинейной активацией.

**Ключевые понятия.** Dataset/DataLoader — загрузка по батчам без полного держания в памяти. `nn.Module`, `forward`, `nn.Parameter`. `loss.backward()` и `optimizer.step()`. `model.eval()` при предсказании.

## Суть для защиты

- Зачем нужны Dataset и DataLoader (батчи, экономия памяти при больших данных).
- Что делает BCEWithLogitsLoss и чем она отличается от потери для обычной регрессии.
- Как устроен один шаг обучения: forward → loss → backward → step; зачем вызывать optimizer.zero_grad().
- Почему на make_moons линейная модель (один слой без активации) даёт ограниченное качество и как нелинейная сеть может его улучшить.
