# Вопросы для самоконтроля — ответы (Лабораторная работа 2)

Ответы на вопросы по классификатору на основе логистической регрессии.

---

## 1. Что представляет из себя модель логистической регрессии?

Модель логистической регрессии — это линейный классификатор, который оценивает **вероятность** принадлежности объекта к одному из классов (в простейшем случае — к классу 1). Входные признаки комбинируются в линейную функцию (взвешенная сумма плюс свободный член); результат этой функции преобразуется сигмоидой в число от 0 до 1, интерпретируемое как вероятность. Таким образом, модель задаётся набором коэффициентов (весов) и порогом: если вероятность класса 1 выше порога (обычно 0,5), объект относят к классу 1, иначе — к классу 0. Модель относится к методам обучения с учителем: коэффициенты подбираются по размеченной выборке путём минимизации функции потерь.

---

## 2. Изобразите график сигмоиды.

Сигмоида (логистическая функция) задаётся формулой σ(z) = 1 / (1 + e^(-z)). График — S-образная кривая: при z → −∞ значение стремится к 0, при z → +∞ — к 1, при z = 0 значение равно 0,5. Кривая монотонно возрастает и симметрична относительно точки (0; 0,5). По горизонтальной оси откладывается z (линейная комбинация признаков), по вертикальной — вероятность. Построить такой график в коде можно, задав сетку по z и вычислив 1/(1+np.exp(-z)), после чего построить линию с помощью matplotlib.

---

## 3. Как рассчитывается выход классификатора? Напишите формулу и правило определения принадлежности объекту классу 0 или 1.

Сначала вычисляется линейная комбинация признаков: **z = b₀ + b₁x₁ + b₂x₂ + … + bₙxₙ**, где x₁, …, xₙ — признаки объекта, b₀ — свободный член, b₁…bₙ — коэффициенты модели. Выход классификатора (вероятность класса 1) вычисляется по формуле сигмоиды: **ŷ = 1 / (1 + e^(-z))**. Правило отнесения к классу: если **ŷ ≥ 0,5**, объект относят к классу 1; если **ŷ < 0,5** — к классу 0. Иными словами, решающая граница задаётся уравнением z = 0 (гиперплоскость в пространстве признаков).

---

## 4. Какая связь между логистической и линейной регрессией?

И в линейной, и в логистической регрессии используется одна и та же **линейная** комбинация признаков z = b₀ + b₁x₁ + … + bₙxₙ. Различие — в том, что с ней делают дальше и какую задачу решают. В линейной регрессии z сам по себе считается предсказанием непрерывной целевой переменной (минимизируется квадратичная ошибка). В логистической регрессии z пропускается через сигмоиду, и получается вероятность класса; модель обучается так, чтобы максимизировать правдоподобие (или минимизировать логистическую функцию потерь) для бинарных меток. Таким образом, логистическая регрессия — это по сути линейная модель с нелинейным (сигмоидальным) преобразованием выхода для задачи классификации.

---

## 5. В каких случаях использование лог. регрессии будет эффективно?

Логистическая регрессия эффективна, когда классы **линейно или приближённо линейно разделимы** в пространстве признаков: существует гиперплоскость, которая с приемлемой точностью отделяет объекты одного класса от другого. Она хорошо работает при достаточном объёме данных, при отсутствии сильной мультиколлинеарности признаков и когда зависимость от признаков по сути линейна (на шкале z). При сильном перекрытии классов или при нелинейных границах (например, один класс внутри другого) эффективность падает; в таких случаях часто используют более сложные модели (деревья решений, ядерные методы, нейросети) или предобработку признаков.

---

## 6. В чем заключается обучение модели лог. регрессии?

Обучение модели логистической регрессии — это **подбор коэффициентов** b₀, b₁, …, bₙ по обучающей выборке. Формально это задача оптимизации: минимизируется функция потерь (чаще всего логистическая, или cross-entropy), которая выражает расхождение между предсказанными вероятностями и истинными метками классов. В результате обучения находятся такие веса, при которых предсказания модели лучше всего согласуются с размеченными данными. На практике оптимизация выполняется итеративными методами (градиентный спуск, SAGA, L-BFGS и т.д.), реализованными в библиотеках вроде scikit-learn; вызов метода **fit()** как раз и выполняет этот процесс.

---

## 7. Опишите принцип градиентного спуска. Есть ли еще методы обучения, применимые к логистической регрессии?

**Градиентный спуск:** на каждом шаге вычисляется градиент функции потерь по параметрам модели; параметры изменяются в направлении, противоположном градиенту (чтобы уменьшить потерю), с некоторым шагом (learning rate). Процесс повторяется до сходимости или до ограничения по числу итераций. Разновидности: полный градиент по всей выборке, стохастический (по одному объекту), мини-батчи и т.д. **SAGA** — вариант стохастического градиентного спуска с регуляризацией, часто используемый в scikit-learn для логистической регрессии.

Другие применимые методы: **L-BFGS** (квази-ньютоновский метод, часто по умолчанию для небольших данных), **Newton-CG**, **liblinear** (для небольших выборок), **saga** и **lbfgs** — все они минимизируют ту же целевую функцию, но разными алгоритмами оптимизации.

---

## 8. Что такое регуляризация? Для чего она используется?

Регуляризация — это добавление к функции потерь **штрафного слагаемого** за величину коэффициентов модели (например, L1 или L2 норма весов). Цель — ограничить сложность модели и уменьшить **переобучение**: модель меньше «подстраивается» под шум в обучающих данных и лучше обобщает на новые. В логистической регрессии в scikit-learn по умолчанию используется L2-регуляризация (параметр C задаёт обратную силу регуляризации). Регуляризация также помогает при мультиколлинеарности и при большом числе признаков.

---

## 9. Почему в формуле потерь используется два слагаемых, а также логарифм?

Формула потерь для бинарной классификации (cross-entropy) записывается в виде суммы по объектам: для объекта с меткой yᵢ и предсказанной вероятностью ŷᵢ вклад в потерь равен −[yᵢ log(ŷᵢ) + (1−yᵢ) log(1−ŷᵢ)]. Два слагаемых нужны потому, что при **yᵢ = 1** мы хотим максимизировать ŷᵢ (минимизировать −log(ŷᵢ)), а при **yᵢ = 0** — минимизировать ŷᵢ (минимизировать −log(1−ŷᵢ)); одна компактная формула покрывает оба случая. Логарифм используется потому, что такая форма соответствует максимизации правдоподобия для схемы Бернулли (бинарный исход), а также даёт удобные с математической точки зрения производные для градиентного спуска и обеспечивает сильный штраф за уверенные, но неверные предсказания (ŷ близко к 0 при y=1 или ŷ близко к 1 при y=0).

---

## 10. Напишите формулы для расчета точности, чувствительности и специфичности классификатора.

- **Точность (accuracy)** = (TP + TN) / (TP + TN + FP + FN) — доля всех верно классифицированных объектов среди всех объектов.

- **Чувствительность (sensitivity, recall для класса 1)** = TP / (TP + FN) — доля верно предсказанных положительных среди всех истинных положительных (класс 1).

- **Специфичность (specificity)** = TN / (TN + FP) — доля верно предсказанных отрицательных среди всех истинных отрицательных (класс 0).

Здесь TP — истинно положительные, TN — истинно отрицательные, FP — ложно положительные, FN — ложно отрицательные; подсчёт ведётся по сравнению предсказанных меток с истинными (обычно с порогом 0,5 по вероятности).
