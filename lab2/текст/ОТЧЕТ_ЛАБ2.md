# Лабораторная работа 2. Классификатор на основе логистической регрессии с градиентным спуском

*(Титульный лист оформляется отдельно по требованиям кафедры.)*

---

## 1. Цель работы

Разработка модели классификатора на основе логистической регрессии, изучение его свойств и принципов работы, получение навыков программирования на Python и использования модуля scikit-learn.

---

## 2. Основные теоретические положения

**Логистическая регрессия** — модель бинарной классификации (классы 0 и 1). Результат работы классификатора ŷ — вероятность наступления события для объекта X (вектор признаков x₁, x₂, …, xₙ), вычисляется по формуле сигмоиды:

ŷ = 1 / (1 + e⁻ᶻ),  
где z = b₀ + b₁x₁ + b₂x₂ + … + bₙxₙ.

Правило отнесения к классу: если ŷ > 0,5 — класс 1, иначе — класс 0. Граница между классами — линейная гиперплоскость (уравнение z = 0).

Метод относится к **обучению с учителем**: по размеченным данным подбираются коэффициенты b₀, b₁, …, bₙ путём минимизации функции потерь. Используемая функция потерь (log loss):

loss = −(1/m) Σ [ yᵢ log(ŷᵢ) + (1 − yᵢ) log(1 − ŷᵢ) ],

где m — число объектов, yᵢ — истинная метка (0 или 1), ŷᵢ — предсказанная вероятность класса 1.

**Градиентный спуск** — итеративное обновление весов в направлении, противоположном градиенту функции потерь, до сходимости. В работе используется алгоритм SAGA (стохастический градиентный спуск с регуляризацией).

**Чувствительность** (истинно положительная пропорция) — доля реально положительных объектов, правильно классифицированных как класс 1: TP / (TP + FN). **Специфичность** (истинно отрицательная пропорция) — доля реально отрицательных, правильно классифицированных как класс 0: TN / (TN + FP). Класс 0 трактуется как отсутствие признака, класс 1 — наличие. **Точность** — доля всех верных предсказаний: (TP + TN) / (TP + TN + FP + FN).

---

## 3. Ход работы

**1.** Создан скрипт, в начале импортированы модули numpy, matplotlib, pathlib и модель LogisticRegression из sklearn.linear_model.

**2.** С использованием функций лабораторной работы №1 (norm_dataset и nonlinear_dataset_5 из lab1.DataGenerator) созданы переменные для трёх экспериментов: Xtrain, Ytrain и Xtest, Ytest. X — объекты двух классов (нормальное распределение для выборок А и Б, нелинейная форма для выборки В), Y — метки класса. Разбиение на обучение и тест — 70% / 30% с перемешиванием.

**3.** Изучена документация по LogisticRegression (scikit-learn).

**4.** Модель обучена на обучающей выборке методом fit(). Для воспроизводимости задан random_state равным номеру варианта (Nvar = 5). Выбран оптимизационный алгоритм SAGA:  
`clf = LogisticRegression(random_state=Nvar, solver='saga').fit(Xtrain, Ytrain)`.

**5.** Получены предсказания для новых данных: predict() — метки классов 0/1; predict_proba() — вероятности принадлежности каждому классу (два столбца). Для гистограмм используется вероятность класса 1 (второй столбец).

**6.** Точность оценена методом score(): acc_train = clf.score(Xtrain, Ytrain), acc_test = clf.score(Xtest, Ytest).

**7.** Точность посчитана вручную: acc_test = sum(Pred_test == Ytest) / len(Ytest).

**8.** Визуализированы результаты: построены гистограммы распределения вероятности принадлежности классу 1 на выходе классификатора для обучающей и тестовой выборок. Объекты с истинной меткой 0 и 1 отображены разными цветами. Гистограммы сохранены в папку figures для выборок А, Б и В (train и test).

**Самостоятельное задание.**  
Рассчитаны чувствительность и специфичность по формулам TP/(TP+FN) и TN/(TN+FP) без использования методов библиотеки (функция sensitivity_specificity). Изменены параметры генерируемых данных для выборки Б (сближены средние, увеличены СКО) — получено более плотное пересечение классов и ухудшение метрик. На выборке В (нелинейно разделимые данные из nonlinear_dataset_5) оценена эффективность классификатора — логистическая регрессия как линейный метод даёт худшие результаты.

---

## 4. Гистограммы распределения вероятности и подписи к рисункам

**Рисунок 1** — Гистограмма распределения вероятности принадлежности классу 1, тестовая выборка А (хорошо разделимые нормальные данные). По оси X — вероятность, по оси Y — число объектов. Два цвета: класс 1 (истинный) и класс 0 (истинный). Гистограммы слабо пересекаются.

**Рисунок 2** — Гистограмма, обучающая выборка А (хорошо разделимые).

**Рисунок 3** — Гистограмма, тестовая выборка Б (плохо разделимые нормальные данные). Пересечение классов по вероятностям больше, много объектов около 0,5.

**Рисунок 4** — Гистограмма, обучающая выборка Б (плохо разделимые).

**Рисунок 5** — Гистограмма, тестовая выборка В (нелинейно разделимые данные). Сильное перекрытие — линейный классификатор ошибается чаще.

**Рисунок 6** — Гистограмма, обучающая выборка В (нелинейно разделимые).

Файлы: `lab2/figures/lab2_hist_test_A.png`, `lab2_hist_train_A.png`, аналогично для B и C.

---

## 5. Таблицы с результатами оценки качества классификации

Формат таблицы (значения подставить из вывода программы после запуска `python -m lab2.main`):

|            | Число объектов | Точность, % | Чувствительность, % | Специфичность, % |
|------------|----------------|-------------|----------------------|-------------------|
| **Train**  | …              | …           | …                    | …                 |
| **Test**   | …              | …           | …                    | …                 |

**Выборка А (нормальное распределение, хорошо разделимые)**  
*(подставить числа из консоли)*

**Выборка Б (нормальное распределение, плохо разделимые)**  
*(подставить числа из консоли)*

**Выборка В (нелинейно разделимые)**  
*(подставить числа из консоли)*

---

## 6. Выводы по работе

В работе реализован классификатор на основе логистической регрессии (scikit-learn) с оптимизатором SAGA. Проведены эксперименты на трёх типах данных: хорошо разделимые нормальные (А), плохо разделимые нормальные (Б), нелинейно разделимые (В).

На выборке А модель даёт высокие точность, чувствительность и специфичность; гистограммы вероятностей для классов 0 и 1 слабо пересекаются. На выборке Б из-за сближения средних и больших СКО метрики снижаются. На выборке В логистическая регрессия как линейный метод оказывается неэффективной — разделяющая граница линейная, а классы нелинейно разделимы (один внутри другого); для таких данных нужны нелинейные методы или ядра.

Чувствительность и специфичность рассчитаны вручную по формулам. Визуализация распределения вероятностей на выходе классификатора позволяет наглядно оценить уверенность модели и области ошибок.

---

## 7. Код программы с пояснениями по этапам

**Импорты и настройка путей.**  
numpy, matplotlib, Path, LogisticRegression. Подключение lab1.DataGenerator (norm_dataset, nonlinear_dataset_5). Создание папки figures.

**Параметры.**  
Nvar = 5 (номер варианта для random_state), N = 1000 (объектов в классе), random_state для воспроизводимости разбиения 70/30.

**Функция sensitivity_specificity(Y_true, Y_pred).**  
По истинным и предсказанным меткам (0/1) вычисляются TP, TN, FP, FN; возвращаются чувствительность = TP/(TP+FN) и специфичность = TN/(TN+FP) с проверкой деления на ноль.

**Функция split_70_30(X, Y).**  
Случайная перестановка индексов с фиксированным seed; разбиение на 70% train и 30% test.

**Функция run_experiment.**  
Обучение LogisticRegression(solver='saga', random_state=Nvar) на Xtrain, Ytrain. Вызов predict и predict_proba для train и test. Расчёт точности (score), чувствительности и специфичности. Построение и сохранение гистограмм вероятности класса 1 для объектов с истинной меткой 0 и 1 (отдельно train и test). Возврат словаря с метриками.

**Функция print_table.**  
Вывод таблицы: число объектов, точность (%), чувствительность (%), специфичность (%) для Train и Test.

**Выборка А.**  
norm_dataset с mu0_A, mu1_A, sigma0_A, sigma1_A (разнесённые средние, умеренные СКО). split_70_30 → run_experiment → print_table.

**Выборка Б.**  
norm_dataset с mu0_B, mu1_B, sigma0_B, sigma1_B (сближенные средние, большие СКО). Аналогично.

**Выборка В.**  
nonlinear_dataset_5(N), разбиение 70/30, run_experiment, print_table.

---

### Файл lab2/main.py (полный код)

```python
"""
Лабораторная работа 2. Классификатор на основе логистической регрессии.
Запуск из корня leti: python -m lab2.main
"""
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.linear_model import LogisticRegression

FIGURES_DIR = Path(__file__).resolve().parent / "figures"
FIGURES_DIR.mkdir(exist_ok=True)

try:
    import lab1.DataGenerator as dg
except ImportError:
    import sys
    _root = Path(__file__).resolve().parent.parent
    if str(_root) not in sys.path:
        sys.path.insert(0, str(_root))
    import lab1.DataGenerator as dg

Nvar = 5
N = 1000
random_state = 42


def sensitivity_specificity(Y_true, Y_pred):
    Y_true = np.asarray(Y_true).ravel()
    Y_pred = np.asarray(Y_pred).ravel()
    Y_true = (Y_true != 0).astype(int)
    Y_pred = (Y_pred != 0).astype(int)
    TP = np.sum((Y_true == 1) & (Y_pred == 1))
    TN = np.sum((Y_true == 0) & (Y_pred == 0))
    FP = np.sum((Y_true == 0) & (Y_pred == 1))
    FN = np.sum((Y_true == 1) & (Y_pred == 0))
    sens = TP / (TP + FN) if (TP + FN) > 0 else 0.0
    spec = TN / (TN + FP) if (TN + FP) > 0 else 0.0
    return sens, spec


def split_70_30(X, Y):
    np.random.seed(random_state)
    idx = np.random.permutation(len(Y))
    X, Y = X[idx], Y[idx]
    train_count = round(0.7 * len(Y))
    return X[:train_count], Y[:train_count], X[train_count:], Y[train_count:]


def run_experiment(Xtrain, Ytrain, Xtest, Ytest, suffix, title_suffix):
    clf = LogisticRegression(random_state=Nvar, solver="saga").fit(Xtrain, Ytrain)
    Pred_train = clf.predict(Xtrain)
    Pred_test = clf.predict(Xtest)
    Pred_train_proba = clf.predict_proba(Xtrain)
    Pred_test_proba = clf.predict_proba(Xtest)
    acc_train = clf.score(Xtrain, Ytrain)
    acc_test = clf.score(Xtest, Ytest)
    sens_train, spec_train = sensitivity_specificity(Ytrain, Pred_train)
    sens_test, spec_test = sensitivity_specificity(Ytest, Pred_test)

    plt.figure()
    plt.hist(Pred_test_proba[Ytest, 1], bins="auto", alpha=0.7, label="Класс 1 (истинный)", color="C0")
    plt.hist(Pred_test_proba[~Ytest, 1], bins="auto", alpha=0.7, label="Класс 0 (истинный)", color="C1")
    plt.xlabel("Вероятность принадлежности классу 1")
    plt.ylabel("Число объектов")
    plt.title(f"Результаты классификации, тест ({title_suffix})")
    plt.legend()
    plt.savefig(FIGURES_DIR / f"lab2_hist_test_{suffix}.png")
    plt.close()

    plt.figure()
    plt.hist(Pred_train_proba[Ytrain, 1], bins="auto", alpha=0.7, label="Класс 1 (истинный)", color="C0")
    plt.hist(Pred_train_proba[~Ytrain, 1], bins="auto", alpha=0.7, label="Класс 0 (истинный)", color="C1")
    plt.xlabel("Вероятность принадлежности классу 1")
    plt.ylabel("Число объектов")
    plt.title(f"Результаты классификации, трейн ({title_suffix})")
    plt.legend()
    plt.savefig(FIGURES_DIR / f"lab2_hist_train_{suffix}.png")
    plt.close()

    return {
        "train": (len(Ytrain), acc_train, sens_train, spec_train),
        "test": (len(Ytest), acc_test, sens_test, spec_test),
    }


def print_table(results, dataset_name):
    print(f"\n--- {dataset_name} ---")
    print(f"{'':8} {'Число объектов':>14} {'Точность, %':>12} {'Чувствительность, %':>20} {'Специфичность, %':>18}")
    print("-" * 78)
    for part in ("train", "test"):
        label = "Train" if part == "train" else "Test"
        n, acc, sens, spec = results[part]
        print(f"{label:8} {n:>14} {acc*100:>11.2f}% {sens*100:>19.2f}% {spec*100:>17.2f}%")


# Выборка А: хорошо разделимые
mu0_A = [0, 2, 3]
mu1_A = [3, 5, 1]
sigma0_A = [2, 1, 2]
sigma1_A = [1, 2, 1]
X_A, Y_A, _, _ = dg.norm_dataset([mu0_A, mu1_A], [sigma0_A, sigma1_A], N)
Xtrain_A, Ytrain_A, Xtest_A, Ytest_A = split_70_30(X_A, Y_A)
results_A = run_experiment(Xtrain_A, Ytrain_A, Xtest_A, Ytest_A, "A", "выборка А")
print_table(results_A, "Выборка А (хорошо разделимые)")

# Выборка Б: плохо разделимые
mu0_B = [1, 3, 2]
mu1_B = [2, 4, 3]
sigma0_B = [2.5, 2, 2.5]
sigma1_B = [2, 2.5, 2]
X_B, Y_B, _, _ = dg.norm_dataset([mu0_B, mu1_B], [sigma0_B, sigma1_B], N)
Xtrain_B, Ytrain_B, Xtest_B, Ytest_B = split_70_30(X_B, Y_B)
results_B = run_experiment(Xtrain_B, Ytrain_B, Xtest_B, Ytest_B, "B", "выборка Б")
print_table(results_B, "Выборка Б (плохо разделимые)")

# Выборка В: нелинейно разделимые
X_C, Y_C, _, _ = dg.nonlinear_dataset_5(N)
Xtrain_C, Ytrain_C, Xtest_C, Ytest_C = split_70_30(X_C, Y_C)
results_C = run_experiment(Xtrain_C, Ytrain_C, Xtest_C, Ytest_C, "C", "выборка В")
print_table(results_C, "Выборка В (нелинейно разделимые)")

print(f"\nГистограммы сохранены в папке: {FIGURES_DIR}")
```
