# Лабораторная работа 2. Классификатор на основе логистической регрессии — подробное объяснение

## 1. Цель работы

Разработка модели **классификатора** на основе логистической регрессии, изучение его свойств и принципов работы, получение навыков программирования на Python и использования модуля **scikit-learn**.

---

## 2. Основные теоретические положения

### 2.1 Что такое логистическая регрессия

**Логистическая регрессия** — это модель **бинарной классификации** (два класса: 0 и 1). Результат работы модели — не «класс», а **вероятность** принадлежности объекту классу 1. По этой вероятности затем принимается решение: если вероятность > 0.5 → класс 1, иначе → класс 0.

### 2.2 Формула сигмоиды (выход классификатора)

Для объекта **X** = (x₁, x₂, …, xₙ) выход модели вычисляется так:

**Линейная комбинация (логит):**
$$z = b_0 + b_1 x_1 + b_2 x_2 + \ldots + b_n x_n$$

**Сигмоида (вероятность):**
$$\hat{y} = \frac{1}{1 + e^{-z}}$$

- **b₀** — свободный член (intercept).
- **b₁…bₙ** — коэффициенты при признаках (веса).
- **z** может быть любым числом; после подстановки в формулу **ŷ** всегда лежит в интервале (0, 1), то есть это вероятность.

**Правило классификации:**
- если **ŷ > 0.5** → предсказываем класс **1**;
- если **ŷ ≤ 0.5** → предсказываем класс **0**.

Граница между классами задаётся уравнением **z = 0** (линейная гиперплоскость в пространстве признаков).

### 2.3 Связь с линейной регрессией

- **Линейная регрессия:** выход = z (линейная функция), используется для **регрессии** (предсказание числа).
- **Логистическая регрессия:** выход = σ(z) = 1/(1+e⁻ᶻ). Та же линейная комбинация z, но результат пропущен через **сигмоиду**, чтобы получить вероятность. Используется для **классификации**.

То есть «логистическая регрессия» — это по сути линейная модель + сигмоида на выходе.

### 2.4 Обучение с учителем и функция потерь

Модель относится к **обучению с учителем**: у нас есть размеченные данные (X, Y), и мы подбираем коэффициенты **b₀, b₁, …, bₙ** так, чтобы минимизировать ошибку предсказания.

**Функция потерь (cross-entropy / log loss)** для бинарной классификации:

$$\text{loss} = -\frac{1}{m} \sum_{i=1}^{m} \Bigl[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \Bigr]$$

где:
- **m** — число объектов в выборке;
- **yᵢ** — истинная метка (0 или 1);
- **ŷᵢ** — предсказанная вероятность класса 1 для объекта i.

Почему два слагаемых и логарифм:
- Для объекта класса 1 (yᵢ=1) второе слагаемое обнуляется, остаётся −log(ŷᵢ). Чтобы loss был меньше, нужно, чтобы ŷᵢ было ближе к 1.
- Для объекта класса 0 (yᵢ=0) первое слагаемое обнуляется, остаётся −log(1−ŷᵢ). Чтобы loss был меньше, нужно, чтобы ŷᵢ было ближе к 0.
- Логарифм даёт удобную для оптимизации форму (гладкая, выпуклая для логистической регрессии) и строго штрафует уверенные неправильные предсказания.

### 2.5 Градиентный спуск

**Градиентный спуск** — метод обучения: мы итеративно обновляем веса в направлении, противоположном градиенту функции потерь, чтобы уменьшить loss.

- Вычисляем градиент loss по весам.
- Делаем шаг: `веса = веса − шаг_обучения × градиент`.
- Повторяем до сходимости.

Варианты в scikit-learn:
- **SAGA** — стохастический градиентный спуск с регуляризацией; часто хороший выбор для классификации.
- Другие: L-BFGS, liblinear, newton-cg и т.д.

### 2.6 Регуляризация

**Регуляризация** — штраф за слишком большие веса. Добавляется к функции потерь (например, L2: λ‖w‖²). Это:
- уменьшает переобучение;
- стабилизирует решение.

В `LogisticRegression` задаётся параметром `C` (обратная сила регуляризации: меньше C — сильнее регуляризация).

### 2.7 Чувствительность и специфичность

При интерпретации: **класс 0** = отсутствие признака (например, «здоров»), **класс 1** = наличие признака (например, «болен»).

- **Чувствительность (Sensitivity, TPR):** доля реально положительных, которых модель правильно отнесла к классу 1.  
  $$\text{Чувствительность} = \frac{TP}{TP + FN}$$

- **Специфичность (Specificity, TNR):** доля реально отрицательных, которых модель правильно отнесла к классу 0.  
  $$\text{Специфичность} = \frac{TN}{TN + FP}$$

где:
- **TP** — истинно положительные (истина 1, предсказание 1);
- **TN** — истинно отрицательные (истина 0, предсказание 0);
- **FP** — ложно положительные (истина 0, предсказание 1);
- **FN** — ложно отрицательные (истина 1, предсказание 0).

**Точность (accuracy):**
$$\text{Точность} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{\text{число верных предсказаний}}{\text{всего объектов}}.$$

---

## 3. Пошаговое объяснение кода

### Шаг 1. Импорты

```python
import numpy as np
from sklearn.linear_model import LogisticRegression
```
- **numpy** — работа с массивами и числами.
- **LogisticRegression** — модель логистической регрессии в scikit-learn.

### Шаг 2. Данные из лабораторной №1

Используются функции из `lab1.DataGenerator`:
- **norm_dataset(mu, sigma, N)** — генерирует два класса с нормальным распределением по признакам; **mu** — списки средних для класса 0 и 1, **sigma** — СКО. Чем ближе средние и чем больше СКО, тем сильнее пересечение классов.
- **nonlinear_dataset_5(N)** — генерирует нелинейно разделимые данные (класс 1 — овал, класс 0 — U-образная оболочка).

Создаются переменные: **Xtrain, Ytrain** (обучение) и **Xtest, Ytest** (тест). Разбиение 70% / 30% делается функцией `split_70_30`.

### Шаг 3–4. Обучение модели

```python
clf = LogisticRegression(random_state=Nvar, solver='saga').fit(Xtrain, Ytrain)
```
- **random_state=Nvar** — фиксирует генератор случайных чисел для воспроизводимости.
- **solver='saga'** — алгоритм оптимизации (стохастический градиентный спуск с регуляризацией).
- **fit(Xtrain, Ytrain)** — обучение (подбор коэффициентов) по обучающей выборке.

### Шаг 5. Предсказания

- **predict(Xtest)** — возвращает предсказанные **метки классов** (0 или 1) по правилу ŷ > 0.5.
- **predict_proba(Xtest)** — возвращает **вероятности** для каждого класса: столбец 0 — P(класс 0), столбец 1 — P(класс 1). Сумма по строке = 1.

Для гистограмм берётся вероятность класса 1: `Pred_test_proba[:, 1]`.

### Шаг 6–7. Оценка точности

- **clf.score(X, Y)** — доля верных ответов (accuracy).
- Вручную: `accuracy = sum(Pred_test == Ytest) / len(Ytest)`.

### Шаг 8. Визуализация

Строятся гистограммы распределения **вероятности принадлежности классу 1** для объектов теста (и трейна):
- одна серия — объекты с истинной меткой 1;
- вторая — с истинной меткой 0.

Хорошо разделимые классы: гистограммы слабо пересекаются (одна ближе к 0, другая к 1). Плохо разделимые или нелинейные — больше пересечение, больше ошибок.

### Самостоятельно: чувствительность и специфичность

В коде они считаются по формулам без методов библиотеки:
- по **Y_true** и **Y_pred** (0/1) находятся TP, TN, FP, FN;
- чувствительность = TP / (TP + FN);
- специфичность = TN / (TN + FP).

Учтено деление на ноль (если в классе нет объектов).

---

## 4. Три выборки и таблицы в отчёте

| Выборка | Описание | Ожидание |
|--------|----------|----------|
| **А** | Нормальное распределение, хорошо разделимые классы (далеко средние, умеренные СКО) | Высокая точность, чувствительность и специфичность |
| **Б** | Нормальное распределение, плохо разделимые (близкие средние и/или большие СКО) | Метрики ниже, гистограммы сильнее пересекаются |
| **В** | Нелинейно разделимые данные (U и овал) | Логистическая регрессия — линейный классификатор, поэтому на нелинейных данных эффективность ещё ниже |

В отчёте нужны:
- гистограммы распределения вероятности для **train** и **test** по всем трём выборкам;
- таблицы: число объектов, точность %, чувствительность %, специфичность % для Train и Test по выборкам А, Б, В.

---

## 5. Ответы на вопросы для самоконтроля

**1. Что представляет из себя модель логистической регрессии?**  
Модель бинарной классификации. Вычисляет линейную комбинацию z = b₀ + b₁x₁ + … + bₙxₙ и преобразует её сигмоидой в вероятность P(класс 1). По этой вероятности (порог 0.5) выдаётся метка 0 или 1.

**2. Изобразите график сигмоиды.**  
График функции σ(z) = 1/(1+e⁻ᶻ): S-образная кривая, при z→−∞ значение стремится к 0, при z→+∞ — к 1, при z=0 значение 0.5. Симметрична относительно точки (0, 0.5).

**3. Как рассчитывается выход классификатора? Правило класса 0/1.**  
Выход: z = b₀ + b₁x₁ + … + bₙxₙ, затем ŷ = 1/(1+e⁻ᶻ). Правило: если ŷ > 0.5 → класс 1, иначе → класс 0.

**4. Какая связь между логистической и линейной регрессией?**  
И там и там используется одна и та же линейная комбинация z. В линейной регрессии выход — само z (регрессия). В логистической выход — σ(z), то есть вероятность (классификация). То есть логистическая регрессия = линейная модель + сигмоида на выходе.

**5. В каких случаях использование лог. регрессии будет эффективно?**  
Когда классы **линейно разделимы** или хотя бы приблизительно разделимы одной гиперплоскостью. При сильном нелинейном разделении эффективность падает (как в выборке В).

**6. В чём заключается обучение модели лог. регрессии?**  
Подбор коэффициентов b₀, b₁, …, bₙ путём минимизации функции потерь (например, log loss) на обучающей выборке с помощью оптимизационного алгоритма (градиентный спуск, SAGA и т.д.).

**7. Опишите принцип градиентного спуска. Другие методы обучения?**  
Итеративно двигаем веса в направлении, противоположном градиенту функции потерь, с некоторым шагом обучения, пока loss не перестанет существенно уменьшаться. Другие методы в sklearn: L-BFGS, liblinear, newton-cg, sag и т.д.

**8. Что такое регуляризация? Для чего она используется?**  
Добавление штрафа за большие веса (например, L2) к функции потерь. Используется для снижения переобучения и повышения обобщающей способности модели.

**9. Почему в формуле потерь два слагаемых и логарифм?**  
Два слагаемых: для объектов класса 1 минимизируем −log(ŷ), для класса 0 — −log(1−ŷ), чтобы подтягивать ŷ к истинной метке. Логарифм даёт гладкую выпуклую функцию и сильно штрафует уверенные неправильные предсказания.

**10. Формулы точности, чувствительности и специфичности.**  
- Точность = (TP + TN) / (TP + TN + FP + FN).  
- Чувствительность = TP / (TP + FN).  
- Специфичность = TN / (TN + FP).

---

## 6. Краткое содержание отчёта

1. **Титульный лист, цель работы, теория** — цель, сигмоида, loss, градиентный спуск, чувствительность/специфичность.
2. **Полный код с пояснениями** — по этапам (импорты, данные, обучение, предсказания, метрики, гистограммы).
3. **Гистограммы** — распределение вероятности класса 1 для train и test по выборкам А, Б, В.
4. **Таблицы** — число объектов, точность, чувствительность, специфичность (Train/Test) для А, Б, В.
5. **Выводы** — как качество зависит от разделимости и линейности; почему на выборке В логистическая регрессия работает хуже.

Если нужно, могу отдельно выписать только ответы на вопросы или шаблон таблицы для отчёта.
