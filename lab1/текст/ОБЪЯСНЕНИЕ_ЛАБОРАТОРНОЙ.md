# Подробное объяснение лабораторной работы 1

Этот файл объясняет лабораторную **простым языком**, без предположения, что вы уже знаете термины. Каждое понятие разобрано по шагам.

---

## 0. Самые базовые вещи (если ничего не знаете)

- **Массив (array)** — таблица чисел: строки и столбцы. Одна строка = один объект (например, один пациент), один столбец = один признак (например, возраст, вес). В NumPy это тип `numpy.ndarray`.
- **Функция** — кусок кода с именем, который принимает параметры (аргументы) и возвращает результат. Например, `norm_dataset(mu, sigma, N)` принимает параметры и возвращает `X, Y, class0, class1`.
- **Импорт** — подключение чужого кода (модуля), чтобы использовать его функции. `import numpy as np` значит: «подключи библиотеку numpy и называй её np».
- **Случайные числа** — компьютер может генерировать «случайные» числа по заданному правилу (распределению). Мы просим, например: «дай 1000 чисел с таким средним и таким разбросом» — и получаем выборку для одного признака одного класса.

Дальше все термины лабораторной разобраны подробнее.

---

## 1. Зачем вообще эта работа?

**Цель в двух словах:** научиться создавать **искусственные данные** для проверки алгоритмов классификации и кластеризации.

**Что это значит по-человечески:**

- В жизни у нас есть объекты (пациенты, картинки, товары) и мы хотим **автоматически** относить их к группам (классам): например, «здоров / болен», «кошка / собака».
- Чтобы проверить, хорошо ли работает такой алгоритм, нужны **данные с известным ответом**: мы сами придумываем данные и сами знаем, к какому классу каждый объект относится. Тогда мы можем сравнить ответ алгоритма с правильным и понять, подходит ли метод.
- В этой работе вы **генерируете** такие данные с помощью кода: задаёте правила (средние, разброс, форму), а компьютер создаёт массивы чисел и метки классов. Дальше эти данные можно использовать для обучения и тестирования классификаторов.

**Используемые модули:**

- **numpy** — работа с массивами чисел (матрицы, векторы), генерация случайных чисел.
- **matplotlib** — рисование графиков (гистограммы, точки на плоскости).

---

## 2. Основные термины (с нуля)

### 2.1 Объект и признаки

- **Объект** — одна «строка» данных. Например, один пациент, одна картинка.
- **Признак** — одно измеряемое свойство объекта (возраст, вес, яркость пикселя и т.д.). В коде каждый признак — это **столбец** в таблице данных.
- **Пространство признаков** — когда признаков несколько, каждый объект можно представить точкой в многомерном пространстве (для 2 признаков — плоскость, для 3 — куб и т.д.).

### 2.2 Класс и метка класса

- **Класс** — группа объектов с общим смыслом (например, «класс 0» — здоровые, «класс 1» — больные).
- **Метка класса (label)** — число или значение, которое говорит, к какому классу относится объект. В нашей работе: 0 для первого класса, 1 для второго. Массив меток **Y** должен идти в том же порядке, что и строки в **X**, чтобы по индексу строки можно было понять метку.

### 2.3 Классификация и кластеризация

- **Классификация** — алгоритму дают объекты **с известными метками** (обучающая выборка), он «учится», а потом для новых объектов **предсказывает** метку класса.
- **Кластеризация** — меток нет; алгоритм сам разбивает объекты на группы по похожести.

В этой лабораторной мы **готовим данные** для таких алгоритмов: создаём X (признаки) и Y (метки).

### 2.4 Обучающая и тестовая выборка

- **Обучающая выборка (train)** — часть данных, на которой алгоритм «учится» (подстраивает параметры).
- **Тестовая выборка (test)** — часть данных, которую алгоритм **не видел** при обучении; по ней проверяют, как он работает на новых данных.
- Разделение 70/30 значит: 70% объектов — в обучающую выборку, 30% — в тестовую. Перед разделением данные **перемешивают**, чтобы в обеих частях были объекты обоих классов.

### 2.5 Нормальное распределение

- **Нормальное распределение (гауссово)** — когда значения случайной величины чаще попадают около какого-то «центра» (среднего), а реже — далеко от него. Форма на графике — колокол.
- **mu (μ)** — среднее значение (центр колокола).
- **sigma (σ)** — стандартное отклонение: чем больше sigma, тем «шире» колокол, тем больше разброс значений.
- В биологии и медицине многие величины (рост, вес, концентрации) приближённо распределены нормально, поэтому такое распределение в задании и используется.

### 2.6 Линейная и нелинейная разделимость

- **Линейно разделимые данные** — два класса можно разделить **одной прямой** (в 2D) или одной плоскостью/гиперплоскостью (в 3D и выше). То есть граница между классами — линия/плоскость.
- **Нелинейно разделимые** — так разделить нельзя; нужна кривая или сложная поверхность (например, один класс внутри другого по кругу/эллипсу). Ваша функция `nonlinear_dataset_N` как раз создаёт такие данные.

### 2.7 Гистограмма и скатерограмма

- **Гистограмма** — график, где по горизонтали отложены интервалы значений признака, по вертикали — сколько объектов попало в каждый интервал (частота). Позволяет увидеть форму распределения (например, колокол для нормального).
- **Скатерограмма (scatter plot)** — точки на плоскости: по осям два признака, каждая точка — один объект. По цвету/маркеру можно показать класс. Позволяет увидеть, как классы расположены друг относительно друга и насколько они пересекаются.

---

## 3. Что мы делаем по шагам (соответствие пунктам методички)

### Пункт 1. Импорт NumPy

```python
import numpy as np
```

**Зачем:** чтобы использовать функции NumPy для массивов и случайных чисел. Сокращение `np` — общепринятое имя при импорте.

---

### Пункт 2. Параметры нормального распределения для двух классов

Мы задаём **два класса**. У каждого класса — свой «центр» (mu) и свой «разброс» (sigma) **по каждому признаку**.

Пример (3 признака, как в методичке):

- **mu0, sigma0** — для класса 0: три числа mu (средние по трём признакам) и три sigma (разбросы).
- **mu1, sigma1** — то же для класса 1.

Числа можно менять: если mu классов близки и sigma большие, классы будут сильнее пересекаться; если mu далеко и sigma маленькие — классы проще разделить.

В методичке сказано про **4-мерное пространство** — значит, нужно 4 признака, т.е. списки из 4 элементов, например:

```python
mu0 = [0, 2, 3, 1]
mu1 = [3, 5, 1, 4]
sigma0 = [2, 1, 2, 1]
sigma1 = [1, 2, 1, 2]
```

---

### Пункт 3. Генерация объектов по классам в цикле

**Идея:** для каждого класса создаём таблицу: **N строк** (по числу объектов) и **col столбцов** (по числу признаков). В каждой ячейке — случайное число из нормального распределения с соответствующими mu и sigma для этого класса и этого признака.

- Сначала создаётся **первый столбец** (признак с индексом 0) для класса 0 и класса 1.
- В цикле `for i in range(1, col)` добавляются **остальные столбцы** (признаки 1, 2, 3, …). Нумерация с 1, потому что нулевой столбец уже создан до цикла.
- `np.random.normal(mu, sigma, (N, 1))` даёт массив размера N×1 из нормального распределения с заданными mu и sigma.
- `np.hstack((class0, v0))` «приклеивает» новый столбец `v0` справа к `class0`, так получается таблица с ещё одним признаком.

В итоге **class0** и **class1** — это матрицы размера N×col.

---

### Пункт 4. Метки класса

Нам нужен массив, где для каждого объекта записано: 0 или 1 (класс). Порядок должен совпадать с порядком строк в X: первая строка X — первый объект, его метка — первый элемент Y.

- **Y0** — N меток «0» для объектов класса 0.
- **Y1** — N меток «1» для объектов класса 1.

Тип `bool` (логический) здесь используется как способ хранить 0 и 1; многие классификаторы умеют работать и с целыми числами. Важно не перепутать порядок: сначала все объекты класса 0, потом все класса 1 — и метки в том же порядке.

---

### Пункт 5. Объединение в X и Y

- **X** — все объекты в одной таблице: сначала все строки класса 0, потом все класса 1. Делается через `np.vstack((class0, class1))`: «вертикально» складываем две матрицы. Размер X: (2*N)×col.
- **Y** — сначала все нули (для класса 0), потом все единицы (для класса 1). Затем `.ravel()` превращает матрицу-столбец в **одномерный** массив длины 2*N — так удобнее передавать в функции классификаторов (sklearn и др.).

Порядок при `vstack` должен быть одинаковым для X и Y, иначе метки перепутаются.

---

### Пункт 6. Перемешивание и разбиение 70/30

**Зачем перемешивать:** если оставить порядок «сначала все класс 0, потом все класс 1», то в обучающую выборку могут попасть в основном объекты одного класса, а в тестовую — другого. Алгоритм тогда будет учиться на перекошенной выборке. После перемешивания в обеих частях есть объекты обоих классов.

**Как делаем:**

1. Создаём массив индексов `arr = np.arange(2*N)` — это числа 0, 1, 2, …, 2*N-1.
2. Перемешиваем эти индексы: `rng.shuffle(arr)` — порядок чисел в `arr` случайный (например, было [0,1,2,3,4,5], стало [2,5,0,4,1,3]).
3. Берём строки X и элементы Y **в новом порядке**: `X = X[arr]`, `Y = Y[arr]`. То есть первая строка нового X — это строка X с номером arr[0], вторая — с номером arr[1] и т.д. Метки Y переставляем теми же индексами, поэтому объект и его метка всегда остаются парой.
4. Первые 70% строк — обучающая выборка, остальные 30% — тестовая. Количество обучающих объектов: `trainCount = round(0.7 * 2 * N)`.

**Пример:** если 2*N = 6, после shuffle могло получиться arr = [2,5,0,4,1,3]. Тогда Xtrain = первые 4 строки X в новом порядке, Xtest = последние 2 строки. В обеих частях будут объекты и класса 0, и класса 1.

В коде нужно аккуратно задать срезы: тестовая выборка — это `X[trainCount : 2*N]`, а не `trainCount : N*2+1` (лишний индекс может привести к пустому или неверному срезу в зависимости от языка).

---

### Пункт 7. Визуализация: гистограммы и скатерограмма

**Гистограммы:** для каждого признака (каждого столбца) рисуем два столбчатых графика на одном рисунке — распределение значений в классе 0 и в классе 1. Так видно, насколько признак различает классы и как сильно распределения пересекаются.

**Скатерограмма:** выбираем два признака (например, 0 и 2), по осям откладываем их значения, каждую точку рисуем цветом/маркером класса. Так видна «картинка» в плоскости двух признаков: разделимы ли классы прямой или нет, есть ли смешение.

---

### Пункт 8. Подписи графиков

В коде уже есть примеры:

- `plt.xlabel("...")` — подпись оси X.
- `plt.ylabel("...")` — подпись оси Y.
- `plt.title("...")` — заголовок графика.
- `plt.legend()` — показывает легенду (какой цвет/маркер — какой класс).

Это нужно для читаемости и для отчёта.

---

### Пункт 9. Вынос генерации в DataGenerator.py

**Зачем:** один раз написали функцию генерации данных — используем её в разных скриптах (лабораторные, эксперименты) без копирования кода.

В файле **DataGenerator.py** объявляется функция, например:

```python
def norm_dataset(mu, sigma, N):
```

Внутрь переносится весь код: от вычисления `col` и генерации `class0`/`class1` до перемешивания X и Y. Входные параметры: `mu` (список из двух списков: mu0 и mu1), `sigma` (аналогично sigma0 и sigma1), `N` (число объектов в классе). Функция возвращает `X, Y, class0, class1`.

---

### Пункт 10. Использование функции в основном скрипте

В **main.py** (или вашем основном скрипте):

- Импортируете модуль: `import DataGenerator as dg`.
- Задаёте параметры: `mu = [mu0, mu1]`, `sigma = [sigma0, sigma1]`, `N`, при необходимости `col = len(mu0)`.
- Вызываете: `X, Y, class0, class1 = dg.norm_dataset(mu, sigma, N)`.

Дальше в скрипте работаете с X, Y, class0, class1 как раньше (разбиение на train/test, построение графиков).

---

## 4. Задание для самостоятельной работы: nonlinear_dataset_N

Нужно добавить в **DataGenerator.py** вторую функцию — например, `nonlinear_dataset_5` (или с вашим номером варианта). Она должна генерировать **двумерные** данные (2 признака), расположенные **заданной формой** по варианту (см. приложение А методички). Например, один класс — кольцо, другой — круг внутри; или два класса в виде полос и т.д.

**Идея:** использовать математику (уравнения окружностей, эллипсов, неравенства для областей) и случайную выборку точек (например, генерировать пары (x, y) и оставлять только те, что попадают в нужную область). Так вы получите данные, которые **нельзя** хорошо разделить одной прямой — они нужны для проверки нелинейных методов и для сравнения с линейными классификаторами.

---

## 5. Краткие ответы на вопросы для самоконтроля

1. **Гистограмма** — график распределения: по оси X интервалы значений признака, по оси Y — количество объектов в каждом интервале.
2. У нормального распределения гистограмма похожа на колокол. **Среднее** — примерно центр колокола, **СКО (sigma)** — примерно половина «ширины» колокола на уровне середины высоты.
3. Двумерные данные можно отображать: **скатерограммой** (точки по двум осям), **гистограммой по каждому признаку**, **контурными графиками** плотности, **цветовой картой** в координатах двух признаков.
4. **«Данные линейно разделимы»** — между двумя классами можно провести одну прямую (в 2D) или одну гиперплоскость (в многомерном пространстве) так, что классы оказываются по разные стороны.
5. **Линейные классификаторы** — алгоритмы, которые строят разделяющую границу в виде прямой/гиперплоскости (например, логистическая регрессия, линейный SVM).
6. Выборку делят на подвыборки, чтобы **оценить обобщающую способность**: на одной части алгоритм учится, на другой — проверяется, как он работает на «новых» данных, которых не было при обучении.
7. Обычно есть **обучающая** (train) и **тестовая** (test) подвыборки; возможны также **валидационная** и схемы **кросс-валидации** (несколько раз разное разбиение).
8. **«Обучение классификатора»** — подбор параметров модели по обучающей выборке так, чтобы она лучше предсказывала метки (минимизация ошибки на обучающих данных с учётом регуляризации и т.д.).
9. **Классификация** — метки классов известны, алгоритм учится по ним и предсказывает метку для новых объектов. **Кластеризация** — меток нет, алгоритм сам разбивает объекты на группы по сходству.
10. Если классов больше двух, метки — целые числа: 0, 1, 2, … (или строки/категории). Один объект — одна метка; массив меток Y имеет длину, равную числу объектов, и i-й элемент — метка i-го объекта.

---

Если что-то из этого нужно разобрать ещё проще или по шагам с вашим конкретным кодом — можно разобрать по строкам в ваших файлах (main.py и DataGenerator.py).

---

## 6. Сводка: что мы делаем по порядку

| Шаг | Что делаем | Зачем |
|-----|------------|--------|
| 1 | Импортируем numpy (и matplotlib для графиков) | Чтобы генерировать массивы и случайные числа, рисовать графики |
| 2 | Задаём mu и sigma для двух классов (по каждому признаку) | Определяем «центр» и «разброс» нормального распределения для каждого класса |
| 3 | В цикле генерируем столбцы признаков и собираем матрицы class0, class1 | Получаем таблицы: N строк × col столбцов для каждого класса |
| 4 | Создаём массивы меток Y0 (нули) и Y1 (единицы) | Чтобы потом знать, к какому классу относится каждая строка в X |
| 5 | Складываем class0 и class1 в X, метки в Y, делаем Y одномерным | Один общий массив данных X и один массив меток Y для всех объектов |
| 6 | Перемешиваем индексы и переставляем строки X и Y одинаково; делим на train/test 70/30 | Обучающая и тестовая выборки содержат оба класса; порядок объект–метка не нарушается |
| 7 | Строим гистограммы по каждому признаку и скатерограмму по двум признакам | Визуально оцениваем распределение и пересекаемость классов |
| 8 | Подписываем оси и заголовки графиков | Чтобы было понятно в отчёте |
| 9 | Переносим генерацию в функцию в DataGenerator.py | Переиспользование кода в других скриптах |
| 10 | В main вызываем dg.norm_dataset(...) вместо повторения кода | Основной скрипт короче и понятнее; данные генерируются одной строкой |
