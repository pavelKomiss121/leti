# Вопросы для самоконтроля — ответы

Ответы на вопросы по лабораторной работе 1 (кратко, по существу).

---

## 1. Что представляет собой гистограмма?

Гистограмма — это способ отображения распределения данных по интервалам (бинам). Ось абсцисс разбивается на отрезки, по каждому отрезку строится столбец, высота которого равна числу наблюдений (или частоте), попавших в этот интервал. Таким образом видно, как часто встречаются те или иные значения признака и где сосредоточены данные. В контексте лабораторной по гистограммам разных классов можно судить о пересекаемости классов по данному признаку.

---

## 2. Изобразите гистограмму нормально распределенных данных. Как по ней можно оценить среднее и СКО?

Гистограмма нормального распределения имеет форму «колокола»: один пик в центре и симметричное убывание к краям.

- **Среднее** оценивается по положению центра пика (максимума столбиков) на оси абсцисс — это приближённое значение математического ожидания.
- **СКО (стандартное отклонение)** можно оценить по «ширине» колокола: например, по расстоянию от центра до точек перегиба (где кривая переходит от выпуклости вниз к выпуклости вверх) или по диапазону, в котором сосредоточена большая часть данных (например, около 68% данных лежит в интервале «среднее ± СКО»).

---

## 3. Как можно отобразить распределение двумерных данных? Приведите несколько вариантов.

- **Скатерограмма (диаграмма рассеяния):** каждая точка — объект с координатами по двум признакам; цвет или маркер может обозначать класс. Хорошо видна форма облака и взаимное расположение классов.
- **Гистограмма по одному признаку:** распределение по одной оси при игнорировании второй (или при усреднении по ней).
- **Двумерная гистограмма (2D-гистограмма):** плоскость разбивается на ячейки, цвет или высота ячейки показывает число точек в ней.
- **Изолинии (контуры плотности):** линии равной плотности оценённого распределения; по форме линий видно, где данные концентрируются.

В лабораторной использовались одномерные гистограммы по каждому признаку и скатерограмма по двум выбранным признакам.

---

## 4. Что значит, что «данные линейно разделимы»?

Данные линейно разделимы, если существует такая гиперплоскость (в двумерном случае — прямая), которая разделяет объекты разных классов так, что по одну сторону лежат объекты одного класса, по другую — другого, и ни одна точка не попадает «не на ту» сторону. То есть классы можно корректно разделить одной прямой (или плоскостью в многомерном пространстве). Если облака классов сильно перекрываются или имеют сложную форму (например, один класс внутри другого), данные, как правило, не являются линейно разделимыми.

---

## 5. Какие классификаторы называются линейными?

Линейными называют классификаторы, которые строят разделяющую поверхность в виде гиперплоскости. Решающее правило имеет вид линейной функции от признаков (например, сумма весов на признаки плюс порог). К ним относятся, в частности, метод опорных векторов с линейным ядром (linear SVM), логистическая регрессия, перцептрон, линейный дискриминант Фишера (LDA). Они хорошо работают, когда данные линейно разделимы или хотя бы приблизительно таковы.

---

## 6. С какой целью при разработке классификатора имеющаяся выборка разделяется на подвыборки?

Выборка разделяется для того, чтобы оценить качество классификатора на данных, которые не использовались при его обучении. По обучающей подвыборке подбираются параметры модели; по тестовой (или валидационной) подвыборке измеряют метрики (точность, полнота и т.д.). Так мы проверяем способность модели обобщать на новые объекты, а не «запоминать» обучающую выборку (что ведёт к переобучению).

---

## 7. Что это за подвыборки и какие варианты разделения существуют?

Обычно выделяют **обучающую** (train), **тестовую** (test) и иногда **валидационную** (validation) подвыборки.

- **Обучающая** — на ней строится (обучается) модель.
- **Тестовая** — на ней один раз в конце оценивается итоговое качество.
- **Валидационная** — используется для подбора гиперпараметров или ранней остановки, чтобы не «подглядывать» в тест.

Варианты разделения: фиксированное соотношение (например, 70% train / 30% test, как в лабораторной), k-fold кросс-валидация (данные разбиваются на k блоков, по очереди один блок — тест, остальные — обучение), случайное разбиение с фиксированным seed для воспроизводимости. Важно перемешивать данные перед разбиением, чтобы в каждой подвыборке были представлены оба класса.

---

## 8. Что подразумевается под «обучением классификатора»?

Под обучением классификатора понимают процесс подбора параметров модели по обучающей выборке так, чтобы модель как можно лучше предсказывала метки классов (или минимизировала ошибку/максимизировала целевую функцию). Конкретный механизм зависит от метода: например, подбор весов в логистической регрессии, построение разделяющей гиперплоскости в SVM, построение дерева решений. В результате получается обученная модель, которую затем можно применять к новым объектам с неизвестными метками.

---

## 9. Чем классификация отличается от кластеризации?

При **классификации** каждому объекту присваивается один из заранее заданных классов; обычно есть размеченная выборка (объекты с известными метками), по которой обучают модель. Задача — предсказать класс для новых объектов.

При **кластеризации** классы заранее не заданы: алгоритм сам разбивает объекты на группы (кластеры) по сходству признаков. Разметки нет; цель — обнаружить структуру в данных (например, «естественные» группы). Таким образом, классификация — обучение с учителем, кластеризация — без учителя (в смысле отсутствия меток классов).

---

## 10. Как можно отразить принадлежность объекта классу через массив меток, если классов больше, чем 2?

Можно использовать массив меток, где каждому объекту соответствует одно значение — номер класса (целое число от 0 до K−1 при K классах). Например, массив `Y` длины N: `Y[i]` — класс объекта с индексом i. Альтернатива — **one-hot кодирование**: для каждого объекта хранится вектор длины K, в котором единица стоит в позиции, соответствующей классу, остальные нули. Многие библиотеки (например, для нейросетей) принимают номера классов (0, 1, …, K−1); при необходимости one-hot строится из них автоматически.
