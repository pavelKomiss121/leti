# Вопросы для самоконтроля — ответы (Лабораторная работа 4)

Ответы на вопросы по нейронным сетям (искусственный нейрон, обучение, метрики, экспорт).

---

## 1. Что такое искусственный нейрон?

**Искусственный нейрон** — вычислительная единица, имитирующая упрощённую модель биологического нейрона. На вход поступают сигналы x₁, x₂, …, xₙ; каждый умножается на **весовой коэффициент** wᵢ; все произведения суммируются (при необходимости добавляется смещение b). Полученная **взвешенная сумма** (net) подаётся на **функцию активации**; результат функции — **выход нейрона**. Таким образом, нейрон по сути вычисляет выход по формуле: выход = σ(Σ xᵢwᵢ + b), где σ — функция активации (например, сигмоида).

---

## 2. Что такое нейронная сеть?

**Нейронная сеть** — совокупность искусственных нейронов, объединённых в **слои**. Сигналы последовательно проходят от входа через один или несколько скрытых слоёв к выходному слою. В **однослойной** сети входы напрямую соединены с выходными нейронами; при наличии **скрытых слоёв** выходы одного слоя служат входами следующего. Обучение сети — подбор весов (и смещений) так, чтобы выходы с минимальной ошибкой соответствовали целевым значениям. В данной работе реализована сеть с одним скрытым слоем и одним выходным нейроном.

---

## 3. Как рассчитывается взвешенная сумма?

**Взвешенная сумма** (net) для одного нейрона вычисляется по формуле:

**net = x₁w₁ + x₂w₂ + … + xₙwₙ**

или в векторной форме **net = X · W** (скалярное произведение вектора входов и вектора весов). При наличии **смещения** b: **net = X · W + b**. В матричной форме для множества объектов: строками X являются векторы признаков, тогда net = X @ W даёт вектор (или матрицу) взвешенных сумм для всех объектов.

---

## 4. Как рассчитать выход с перцептрона с функцией активации сигмоидой при известных весах и входах?

При известных входах **X** (вектор или матрица) и весах **W** (и при необходимости смещении b):

1. Вычислить **взвешенную сумму:** net = X·W + b (или net = X @ W для матриц).
2. Применить **сигмоиду** к net: **выход = σ(net) = 1 / (1 + e⁻ⁿᵉᵗ)**.

Итог — число (или вектор) в интервале (0, 1). Для классификации выход сравнивают с порогом (например, ≥ 0.5 → класс 1, иначе класс 0).

---

## 5. Для чего визуализировать графики изменения функции потерь и точности, как их анализировать?

**Зачем визуализировать:** чтобы оценить **ход обучения** — сходится ли модель (потери падают, точность растёт), нет ли колебаний или «застревания», и чтобы **подобрать число эпох** (достаточно ли обучения, не переобучается ли модель при избытке эпох).

**Как анализировать:**  
- **Функция потерь (MSE):** должна в целом **уменьшаться** с ростом номера эпохи; выход на плато говорит о сходимости. Если потери растут или не меняются — возможны ошибка в градиенте или неверный шаг обучения (learning rate).  
- **Точность:** должна **расти** и стабилизироваться. Резкие осцилляции могут указывать на слишком большой learning rate. Сравнение графиков потерь и точности по эпохам помогает выбрать разумное число эпох и при необходимости скорректировать параметры обучения.

---

## 6. Как обучается стандартная НС без скрытых слоёв?

Стандартная однослойная НС (входы напрямую соединены с выходными нейронами) обучается так же, как и многослойная, но с одним набором весов **W** (вход → выход):

1. **Прямой проход:** по текущим весам вычисляется выход: net = X·W (+ b), выход = σ(net).
2. **Вычисление ошибки** (например, MSE между выходом и целевыми метками y).
3. **Обратное распространение:** вычисляется градиент ошибки по весам W (через производную функции потерь по выходу и производную сигмоиды).
4. **Обновление весов:** W обновляются в направлении, уменьшающем ошибку (градиентный спуск: W := W − η·∇L или в варианте с прибавлением градиента при соответствующем знаке в формулах).

Цикл повторяется много раз (эпохи) по всей выборке (или по батчам), пока потери не стабилизируются.

---

## 7. Как получить предсказание предобученной НС на новых данных?

Для предсказания на **новых данных** нужно выполнить только **прямой проход** (feedforward) с матрицей признаков новых объектов **X_new**, **не меняя весов**:

1. Подставить X_new в формулу выхода: layer1 = σ(X_new · W1), output = σ(layer1 · W2) (для сети с одним скрытым слоем).
2. Выход — вектор (матрица) значений в (0, 1). Для получения меток класса применить порог: например, выход ≥ 0.5 → класс 1, иначе класс 0.

В реализованном коде для этого служит метод **test(X)** (или feedforward(X)), который возвращает выход сети для переданной матрицы X без изменения весов и внутреннего состояния обучения.

---

## 8. Какие параметры и характеристики модели необходимо экспортировать, чтобы после обучения использовать её в другом приложении?

Чтобы использовать обученную НС в другом приложении, нужно сохранить:

- **Веса:** все матрицы весов (например, weights1 — вход–скрытый слой, weights2 — скрытый слой–выход) и при наличии **смещения** (bias) — их значения.
- **Структуру сети:** число входов (признаков), число нейронов в каждом скрытом слое, число выходов; тип функции активации (сигмоида и т.д.).
- **Порог классификации** (если фиксирован, например 0.5) и правило перевода выхода в метку класса.

Этих данных достаточно, чтобы в другом приложении воспроизвести прямой проход и получать предсказания для новых объектов.

---

## 9. Какие значения могут быть получены на выходе из НС?

При использовании **сигмоиды** σ(z) = 1/(1+e⁻ᶻ) выход нейрона лежит в **открытом интервале (0, 1)**: значения строго между 0 и 1, крайние значения 0 и 1 не достигаются при конечных z. В задачах бинарной классификации этот выход интерпретируют как **вероятность** или «уверенность» принадлежности классу 1; по порогу (например, 0.5) его переводят в метку класса (0 или 1).

При других функциях активации диапазон выхода другой: например, для линейной активации — любое число; для softmax на выходном слое — вероятности по классам (сумма 1). В данной работе используется только сигмоида, поэтому выход — числа в (0, 1).
