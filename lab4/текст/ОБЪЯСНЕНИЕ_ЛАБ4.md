# Лабораторная работа 4. Нейронные сети — подробное объяснение

## 1. Цель работы

Создание **простейшей нейронной сети** на Python **без специализированных библиотек** (только numpy): реализация искусственного нейрона с функцией активации сигмоидой, прямого распространения (feedforward) и обратного распространения (backprop), обучение по среднеквадратичной ошибке (MSE), расчёт точности, построение графиков потерь и точности от эпохи, подбор числа нейронов и эпох; вынесение весов в отдельные переменные и оценка на тестовой выборке методом test().

---

## 2. Основные теоретические положения

### 2.1 Искусственный нейрон (ИН)

**Искусственный нейрон** — вычислительный блок: на вход подаются сигналы x₁, x₂, …, xₙ; каждый умножается на **весовой коэффициент** wᵢ и все произведения суммируются. Получается **взвешенная сумма** (net). К net может добавляться **смещение** b. Результат net (или net + b) подаётся на **функцию активации** σ; значение σ(net) — **выход нейрона**.

Формула: **net = Σᵢ xᵢwᵢ** (и при наличии смещения: net = Σᵢ xᵢwᵢ + b). **Выход = σ(net)**.

### 2.2 Функции активации

**Единичный скачок:** выход 0, если net ниже порога, и 1, если выше. Простейший вариант.

**Сигмоида** σ(z) = 1 / (1 + e⁻ᶻ). Выход в интервале (0, 1); функция гладкая, что нужно для градиентного спуска. **Производная:** σ'(z) = σ(z)(1 − σ(z)); при уже вычисленном выходе p = σ(z) производная равна **p(1 − p)** — это используется в backprop.

В коде: выходы слоёв считаются по сигмоиде; для устойчивости аргумент ограничивают (например, clip в диапазоне [-500, 500]), чтобы избежать переполнения при больших |z|.

### 2.3 Нейронная сеть (однослойная и с одним скрытым слоем)

**Однослойная сеть:** входы напрямую соединены с выходным слоем нейронов; промежуточных слоёв нет. Входы изображают кружками (не нейроны), выходной слой — квадратами (нейроны).

В данной работе реализована сеть с **одним скрытым слоем**: входной вектор → скрытый слой (несколько нейронов с сигмоидой) → один выходной нейрон (сигмоида). Это уже двухслойная сеть по весам (два набора весов: вход–скрытый и скрытый–выход). Входы по-прежнему не считаются «слоем нейронов».

**Прямое распространение:** по текущим весам последовательно вычисляются выходы каждого слоя; выход последнего слоя — предсказание сети (число от 0 до 1).

### 2.4 Обучение нейронной сети

**Цель обучения** — подобрать веса так, чтобы выход сети с минимальной ошибкой приближался к целевым меткам (0 или 1 для двух классов).

**Функция потерь** — MSE: (1/n) Σ (y − ŷ)². Минимизация MSE выполняется **градиентным спуском**: на каждом шаге веса изменяются в направлении, противоположном градиенту потерь по весам. Формулы градиента получают по цепному правилу — это и есть **обратное распространение ошибки (backprop)**.

**Один проход по всей выборке** с расчётом градиентов и обновлением весов — одна **эпоха**. Обучение продолжается заданное число эпох (или до сходимости по loss/точности).

### 2.5 Прямое и обратное распространение в реализованной сети

**Прямое:**  
layer1 = σ(X · W1), output = σ(layer1 · W2).  
Размерности: X (n, n_inp), W1 (n_inp, n_neuro), layer1 (n, n_neuro); W2 (n_neuro, 1), output (n, 1).

**Обратное (схематично):**  
Градиент потерь по выходу: dL/d(output) ∝ (y − output). Умножаем на производную сигмоиды по выходу и распространяем назад.  
d_weights2 = layer1ᵀ · (градиент по выходу);  
градиент по layer1 = (градиент по выходу) · W2ᵀ, умножаем на σ'(layer1);  
d_weights1 = Xᵀ · (градиент по layer1).  
Обновление: weights += learning_rate · d_weights (в методичке используется прибавление градиента к весам при соответствующем знаке в формулах).

### 2.6 Точность и использование выхода сети

Выход сети — число в (0, 1). **Класс** обычно получают по порогу: если выход ≥ 0.5, то класс 1, иначе 0. **Точность** — доля объектов, у которых предсказанный класс совпал с истинным. Для предсказания на новых данных вызывают feedforward (или метод test) с матрицей признаков X и по выходу применяют порог.

---

## 3. Пошаговое объяснение кода

### Шаг 1. Импорты и модули (п.1–2 методички)

**lab4/model.py:** numpy; функции sigmoid(Z) с clip и sigmoid_derivative(p) = p*(1−p). Класс NeuralNetwork с __init__, feedforward, backprop, train_step, test.

**lab4/utils.py:** accuracy (приведение pred_proba к классам по порогу 0.5, сравнение с y_true), mse_loss (среднее (y−pred)²), split_70_30 (перемешивание, разбиение 70%/30%).

**lab4/main.py:** numpy, matplotlib, Path; импорт NeuralNetwork из lab4.model, accuracy, mse_loss, split_70_30 из lab4.utils; подключение lab1.DataGenerator (norm_dataset).

### Шаг 2. Данные (п.5 методички)

Параметры mu0, mu1, sigma0, sigma1, N. Вызов dg.norm_dataset(mu, sigma, N) → X, Y_flat, class0, class1. Y приводится к форме (n_samples, 1) и к типу float64 для вычислений. Данные уже перемешаны генератором.

### Шаг 3. Инициализация сети и цикл обучения (п.4, 6 методички)

Создаётся объект NeuralNetwork(X, Y, n_neuro=N_NEURONS, learning_rate=LEARNING_RATE, seed=SEED). В цикле по эпохам: pred = NN.feedforward(); loss = mse_loss(Y, pred); acc = accuracy(Y, pred); запись в history_loss и history_acc; NN.train_step(X, Y). Таким образом, на каждой эпохе метрики считаются до обновления весов (по текущему состоянию сети), затем веса обновляются.

### Шаг 4. Итоговая точность, веса, графики (п.2, 5 самостоятельной, п.3)

После цикла: pred_final = NN.feedforward(); вывод итоговой точности и MSE. weights_layer1 = NN.weights1.copy(), weights_layer2 = NN.weights2.copy(); вывод массивов весов. Строятся два графика (MSE и точность от номера эпохи), сохраняются в lab4/figures/lab4_loss_accuracy.png.

### Шаг 5. Оценка на тестовой выборке (доп. задание «со звёздочкой»)

Xtrain, Ytrain, Xtest, Ytest = split_70_30(X, Y). Создаётся новая сеть NN_split, обучается на (Xtrain, Ytrain) то же число эпох. pred_test = NN_split.test(Xtest) — предсказание на тесте без изменения весов. Вывод точности на обучающей (по feedforward на Xtrain) и на тестовой (по pred_test).

### Шаг 6. Подбор числа нейронов и эпох (п.4 самостоятельной)

Вложенные циклы по n_neur (2, 4, 8, 16) и n_ep (30, 50, 100). Для каждой пары создаётся новая сеть, обучается на (X, Y) n_ep эпох, считается точность на Y по feedforward(). Запоминается комбинация с максимальной точностью; результат выводится в консоль.

---

## 4. Анализ графиков потерь и точности

**Зачем визуализировать:** чтобы убедиться, что обучение идёт (MSE уменьшается, точность растёт), подобрать число эпох (до выхода на плато) и заметить нестабильность (сильные колебания) или отсутствие сходимости.

**Как анализировать:**  
- MSE должен в целом уменьшаться; если растёт или не меняется — проверить знак градиента и шаг обучения.  
- Точность должна расти и стабилизироваться; плато говорит о том, что добавлять эпохи может быть бесполезно.  
- Резкие осцилляции — признак слишком большого learning rate.  
- По графикам выбирают разумное число эпох и при необходимости сравнивают разные числа нейронов в блоке подбора.

---

## 5. Что экспортировать для использования модели в другом приложении

Чтобы использовать обученную сеть в другом приложении, необходимо сохранить:

- **Веса:** weights1 (вход → скрытый слой), weights2 (скрытый → выход). Форма и порядок элементов должны совпадать с ожидаемыми при загрузке.
- **Структуру:** число входов (признаков), число нейронов скрытого слоя, тип активации (сигмоида). При наличии смещений — их значения.
- **Порог классификации** (если фиксирован, например 0.5) и способ приведения выхода к классу.

Этого достаточно, чтобы воспроизвести прямой проход (feedforward) и получить предсказание на новых данных.

---

## 6. Краткие ответы на типичные вопросы

**Что такое искусственный нейрон?**  
Вычислительная единица: взвешенная сумма входов подаётся на функцию активации; результат — выход нейрона.

**Как считается взвешенная сумма?**  
net = Σ xᵢwᵢ (плюс смещение b при его наличии).

**Как рассчитать выход перцептрона с сигмоидой при известных весах и входах?**  
Вычислить net = X·W (и при необходимости добавить b), затем выход = σ(net) = 1/(1+e⁻ⁿᵉᵗ).

**Как обучается НС без скрытых слоёв?**  
Так же: прямой проход по весам до выхода, расчёт ошибки (MSE), обратное распространение градиента по весам, обновление весов. Скрытых слоёв нет — один набор весов от входа к выходу.

**Как получить предсказание предобученной НС на новых данных?**  
Вызвать прямой проход (feedforward или test) с матрицей признаков новых объектов; к выходу применить порог (например, ≥ 0.5 → класс 1).

**Какие значения на выходе НС?**  
При сигмоиде — числа в интервале (0, 1). Для бинарной классификации их переводят в классы по порогу.

---

Используйте этот файл для подготовки к защите и для вставки теоретических фрагментов в отчёт. После запуска `python -m lab4.main` подставьте в отчёт численные результаты из консоли (точность, MSE, веса, лучшую пару нейроны/эпохи).
